{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "08be2447",
   "metadata": {
    "id": "08be2447",
    "outputId": "1e88c739-7fa9-4ae5-b9c0-1cef77384687"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anusseth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "### need a local attention_local.py file for this.\n",
    "from attention_local import AttentionLayer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "eac12a39",
   "metadata": {
    "id": "eac12a39"
   },
   "outputs": [],
   "source": [
    "class Text_Summarization():\n",
    "    def __init__(self, num_rows):\n",
    "        self.num_rows = num_rows\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.run_process()\n",
    "\n",
    "\n",
    "    def run_process(self):\n",
    "        self.data, self.cleaned_text, self.cleaned_summary = self.data_import_preprocess()\n",
    "        self.x_tr, self.y_tr, self.x_val, self.y_val, self.x_voc, self.y_voc = self.cleanedData()\n",
    "        #self.x_tr, self.x_val, self.x_voc = self.tokenizer(train, val)\n",
    "#         self.text_word_count = self.cleaned_data_text(self.cleaned_text)\n",
    "#         self.summary_word_count = self.cleaned_data_summary(self.cleaned_summary)\n",
    "\n",
    "    def text_cleaner(self, text):\n",
    "        newString = text.lower()\n",
    "        newString = BeautifulSoup(newString, \"lxml\").text\n",
    "        newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "        newString = re.sub('\"','', newString)\n",
    "        newString = ' '.join([contractions.fix(t) for t in newString.split(\" \")])\n",
    "        newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "        newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "        newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "\n",
    "        tokens = [w for w in newString.split() if not w in self.stopwords]\n",
    "\n",
    "        long_words=[]\n",
    "        for i in tokens:\n",
    "            if len(i)>1:               #removing short word\n",
    "                long_words.append(i)\n",
    "        return (\" \".join(long_words)).strip()\n",
    "\n",
    "\n",
    "\n",
    "    def data_import_preprocess(self):\n",
    "    #Loading the data from the Amazon Review csv\n",
    "        data = pd.read_csv(r\".\\amazon_food_reviews\\Reviews.csv\", nrows = self.num_rows)\n",
    "\n",
    "\n",
    "        data.drop(columns = ['ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time'], inplace=True) #drop useless columns\n",
    "        data.drop_duplicates(subset = ['Text'], inplace=True) #dropping duplicates\n",
    "        data.replace('', np.nan, inplace=True)\n",
    "        data.dropna(axis=0, inplace = True) #dropping na\n",
    "\n",
    "        cleaned_text = []\n",
    "        for t in data['Text']:\n",
    "            cleaned_text.append(self.text_cleaner(t))\n",
    "        cleaned_summary = []\n",
    "        for t in data['Summary']:\n",
    "            cleaned_summary.append(self.text_cleaner(t))\n",
    "\n",
    "        data['cleaned_text'], data['cleaned_summary'] = cleaned_text, cleaned_summary\n",
    "\n",
    "        return data, cleaned_text, cleaned_summary\n",
    "    \n",
    "    def majority_length_data(self, lst, cleaned_data):\n",
    "#         starttime = time.time()\n",
    "        dict_summary = {item: lst.count(item) for item in set(lst)}\n",
    "#         endtime = time.time()\n",
    "#         print(\"time diff is\", endtime - starttime)\n",
    "       # dict_summary = {item: lst(count for count in lst).count(item) for item in sorted(set(lst))}\n",
    "\n",
    "#         print(dict_summary)\n",
    "\n",
    "        percent = round(0.95 * len(cleaned_data))\n",
    "        print(percent)\n",
    "\n",
    "        count = 0\n",
    "        for key, value in sorted(dict_summary.items()):\n",
    "            count += value\n",
    "            if count>=percent:\n",
    "              return key\n",
    "    def tokenizer(self, train, val):\n",
    "        x_tokenizer = Tokenizer()\n",
    "        x_tokenizer.fit_on_texts(list(train))\n",
    "        thresh=6\n",
    "\n",
    "        cnt=0\n",
    "        tot_cnt=0\n",
    "        freq=0\n",
    "        tot_freq=0\n",
    "\n",
    "        for key,value in x_tokenizer.word_counts.items():\n",
    "            tot_cnt=tot_cnt+1\n",
    "            tot_freq=tot_freq+value\n",
    "            if(value<thresh):\n",
    "                cnt=cnt+1\n",
    "                freq=freq+value\n",
    "\n",
    "        print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "        print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n",
    "        #prepare a tokenizer for reviews on training data\n",
    "        x_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
    "        x_tokenizer.fit_on_texts(list(train))\n",
    "\n",
    "        #convert text sequences into integer sequences\n",
    "        x_tr_seq    =   x_tokenizer.texts_to_sequences(train)\n",
    "        x_val_seq   =   x_tokenizer.texts_to_sequences(val)\n",
    "\n",
    "\n",
    "        #padding zero upto maximum length\n",
    "        train    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "        val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "        #size of vocabulary ( +1 for padding token)\n",
    "        x_voc   =  x_tokenizer.num_words + 1\n",
    "        #print(x_tokenizer.word_counts[text],len(train))\n",
    "        return x_voc, train, val\n",
    "            \n",
    "        \n",
    "    def emptyRows(self, var):\n",
    "        ind=[]\n",
    "        for i in range(len(var)):\n",
    "            cnt=0\n",
    "            for j in var[i]:\n",
    "                if j!=0:\n",
    "                    cnt=cnt+1\n",
    "            if(cnt==2):\n",
    "                ind.append(i)\n",
    "\n",
    "        return ind\n",
    "        \n",
    "    def cleanedData(self):\n",
    "        text_word_count = []\n",
    "        summary_word_count = []\n",
    "        max_text_len = 0\n",
    "        max_summary_len = 0\n",
    "\n",
    "        # populate the lists with sentence lengths\n",
    "        for i in text_class.data['cleaned_text']:\n",
    "              text_word_count.append(len(i.split()))\n",
    "        \n",
    "\n",
    "        for i in text_class.data['cleaned_summary']:\n",
    "              summary_word_count.append(len(i.split()))\n",
    "\n",
    "        length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "        max_summary_len = self.majority_length_data(summary_word_count, self.cleaned_summary)\n",
    "        max_text_len = self.majority_length_data(text_word_count, self.cleaned_text)\n",
    "        print(max_summary_len)\n",
    "        print(max_text_len)\n",
    "        \n",
    "        cleaned_text =np.array(self.cleaned_text)\n",
    "        cleaned_summary=np.array(self.cleaned_summary)\n",
    "\n",
    "        short_text=[]\n",
    "        short_summary=[]\n",
    "\n",
    "        for i in range(len(cleaned_text)):\n",
    "            if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "                short_text.append(cleaned_text[i])\n",
    "                short_summary.append(cleaned_summary[i])\n",
    "\n",
    "        df=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
    "        df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "        x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True)\n",
    "        x_voc, x_tr, x_val = self.tokenizer(x_tr, x_val)\n",
    "        y_voc, y_tr, y_val = self.tokenizer(y_tr, y_val)\n",
    "        ind = self.emptyRows(y_tr)\n",
    "        y_tr=np.delete(y_tr,ind, axis=0)\n",
    "        x_tr=np.delete(x_tr,ind, axis=0)\n",
    "        ind = self.emptyRows(y_val)\n",
    "        y_val=np.delete(y_val,ind, axis=0)\n",
    "        x_val=np.delete(x_val,ind, axis=0)\n",
    "        return x_tr, y_tr,x_val, y_val, x_voc, y_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "13383c43",
   "metadata": {
    "id": "13383c43",
    "outputId": "beb3ec1c-eb83-4942-a288-a52bedd53692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anusseth\\AppData\\Local\\Temp\\ipykernel_16876\\219917810.py:17: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  newString = BeautifulSoup(newString, \"lxml\").text\n",
      "C:\\Users\\anusseth\\AppData\\Local\\Temp\\ipykernel_16876\\219917810.py:17: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  newString = BeautifulSoup(newString, \"lxml\").text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84004\n",
      "84004\n",
      "6\n",
      "104\n",
      "% of rare words in vocabulary: 71.36603637348256\n",
      "Total Coverage of rare words: 2.2052559981298776\n",
      "% of rare words in vocabulary: 76.49653434152489\n",
      "Total Coverage of rare words: 4.721348808175155\n"
     ]
    }
   ],
   "source": [
    "text_class = Text_Summarization(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9cdb5119",
   "metadata": {
    "id": "9cdb5119",
    "outputId": "5443cec7-7b2d-46f5-a806-d9dfd7d628b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                Summary  \\\n",
       "0   1  Good Quality Dog Food   \n",
       "1   2      Not as Advertised   \n",
       "2   3  \"Delight\" says it all   \n",
       "3   4         Cough Medicine   \n",
       "4   5            Great taffy   \n",
       "\n",
       "                                                Text  \\\n",
       "0  I have bought several of the Vitality canned d...   \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  This is a confection that has been around a fe...   \n",
       "3  If you are looking for the secret ingredient i...   \n",
       "4  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                        cleaned_text        cleaned_summary  \n",
       "0  bought several vitality canned dog food produc...  good quality dog food  \n",
       "1  product arrived labeled jumbo salted peanuts p...             advertised  \n",
       "2  confection around centuries light pillowy citr...           delight says  \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine  \n",
       "4  great taffy great price wide assortment yummy ...            great taffy  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_class.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c16a2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt=0\n",
    "# for i in text_class.data['cleaned_summary']:\n",
    "#     if(len(i.split())<=max_summary_len):\n",
    "#         cnt=cnt+1\n",
    "# print (cnt)\n",
    "# print(cnt/len(text_class.data['cleaned_summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "73eb7ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_tokenizer.word_counts['sostok'],len(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e19c88b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 104)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 104, 100)             1269100   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 104, 300),           481200    ['embedding[0][0]']           \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 104, 300),           721200    ['lstm[0][0]']                \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, None, 100)            298500    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, 104, 300),           721200    ['lstm_1[0][0]']              \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, None, 300),          481200    ['embedding_1[0][0]',         \n",
      "                              (None, 300),                           'lstm_2[0][1]',              \n",
      "                              (None, 300)]                           'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " attention_layer (Attention  ((None, None, 300),          180300    ['lstm_2[0][0]',              \n",
      " Layer)                       (None, None, 104))                     'lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)  (None, None, 600)            0         ['lstm_3[0][0]',              \n",
      "                                                                     'attention_layer[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, None, 2985)           1793985   ['concat_layer[0][0]']        \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5946685 (22.68 MB)\n",
      "Trainable params: 5946685 (22.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "#print(K.tensorflow_backend._get_available_gpus())\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(text_class.x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(text_class.y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(text_class.y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e97ba63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6ed7cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 19/563 [>.............................] - ETA: 7:07:38 - loss: 1.6503"
     ]
    }
   ],
   "source": [
    "history=model.fit([text_class.x_tr,text_class.y_tr[:,:-1]], text_class.y_tr.reshape(text_class.y_tr.shape[0],text_class.y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([text_class.x_val,text_class.y_val[:,:-1]], text_class.y_val.reshape(text_class.y_val.shape[0],text_class.y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4da5c77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2944  1054  1419 ...     0     0     0]\n",
      " [   50   167   648 ...     0     0     0]\n",
      " [ 3028  6295   159 ...     0     0     0]\n",
      " ...\n",
      " [ 3152   225  1279 ...     0     0     0]\n",
      " [  194 10173 11485 ...     0     0     0]\n",
      " [   24    83   546 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c1ef78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
