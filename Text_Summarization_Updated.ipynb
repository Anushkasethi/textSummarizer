{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "08be2447",
   "metadata": {
    "id": "08be2447",
    "outputId": "1e88c739-7fa9-4ae5-b9c0-1cef77384687"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anusseth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "### need a local attention_local.py file for this.\n",
    "from attention_local import AttentionLayer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "eac12a39",
   "metadata": {
    "id": "eac12a39"
   },
   "outputs": [],
   "source": [
    "class Text_Summarization():\n",
    "    def __init__(self, num_rows):\n",
    "        self.num_rows = num_rows\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.run_process()\n",
    "\n",
    "\n",
    "    def run_process(self):\n",
    "        self.data, self.cleaned_text, self.cleaned_summary = self.data_import_preprocess()\n",
    "        self.text_word_count, self.summary_word_count, self.max_summary_len, self.max_text_len = self.cleanedData()\n",
    "        #self.x_tr, self.x_val, self.x_voc = self.tokenizer(train, val)\n",
    "#         self.text_word_count = self.cleaned_data_text(self.cleaned_text)\n",
    "#         self.summary_word_count = self.cleaned_data_summary(self.cleaned_summary)\n",
    "\n",
    "    def text_cleaner(self, text):\n",
    "        newString = text.lower()\n",
    "        newString = BeautifulSoup(newString, \"lxml\").text\n",
    "        newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "        newString = re.sub('\"','', newString)\n",
    "        newString = ' '.join([contractions.fix(t) for t in newString.split(\" \")])\n",
    "        newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "        newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "        newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "\n",
    "        tokens = [w for w in newString.split() if not w in self.stopwords]\n",
    "\n",
    "        long_words=[]\n",
    "        for i in tokens:\n",
    "            if len(i)>1:               #removing short word\n",
    "                long_words.append(i)\n",
    "        return (\" \".join(long_words)).strip()\n",
    "\n",
    "\n",
    "\n",
    "    def data_import_preprocess(self):\n",
    "    #Loading the data from the Amazon Review csv\n",
    "        data = pd.read_csv(r\".\\amazon_food_reviews\\Reviews.csv\", nrows = self.num_rows)\n",
    "\n",
    "\n",
    "        data.drop(columns = ['ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time'], inplace=True) #drop useless columns\n",
    "        data.drop_duplicates(subset = ['Text'], inplace=True) #dropping duplicates\n",
    "        data.replace('', np.nan, inplace=True)\n",
    "        data.dropna(axis=0, inplace = True) #dropping na\n",
    "\n",
    "        cleaned_text = []\n",
    "        for t in data['Text']:\n",
    "            cleaned_text.append(self.text_cleaner(t))\n",
    "        cleaned_summary = []\n",
    "        for t in data['Summary']:\n",
    "            cleaned_summary.append(self.text_cleaner(t))\n",
    "\n",
    "        data['cleaned_text'], data['cleaned_summary'] = cleaned_text, cleaned_summary\n",
    "\n",
    "        return data, cleaned_text, cleaned_summary\n",
    "    \n",
    "    def majority_length_data(self, lst, cleaned_data):\n",
    "        dict_summary = {item: lst.count(item) for item in sorted(lst)}\n",
    "       # dict_summary = {item: lst(count for count in lst).count(item) for item in sorted(set(lst))}\n",
    "\n",
    "        print(dict_summary)\n",
    "\n",
    "        percent = round(0.95 * len(cleaned_data))\n",
    "        print(percent)\n",
    "\n",
    "        count = 0\n",
    "        for key, value in sorted(dict_summary.items()):\n",
    "            count += value\n",
    "            if count>=percent:\n",
    "              return key\n",
    "    def tokenizer(self, train, val):\n",
    "        x_tokenizer = Tokenizer()\n",
    "        x_tokenizer.fit_on_texts(list(train))\n",
    "        thresh=6\n",
    "\n",
    "        cnt=0\n",
    "        tot_cnt=0\n",
    "        freq=0\n",
    "        tot_freq=0\n",
    "\n",
    "        for key,value in x_tokenizer.word_counts.items():\n",
    "            tot_cnt=tot_cnt+1\n",
    "            tot_freq=tot_freq+value\n",
    "            if(value<thresh):\n",
    "                cnt=cnt+1\n",
    "                freq=freq+value\n",
    "\n",
    "        print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "        print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n",
    "        #prepare a tokenizer for reviews on training data\n",
    "        x_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
    "        x_tokenizer.fit_on_texts(list(train))\n",
    "\n",
    "        #convert text sequences into integer sequences\n",
    "        x_tr_seq    =   x_tokenizer.texts_to_sequences(train)\n",
    "        x_val_seq   =   x_tokenizer.texts_to_sequences(val)\n",
    "\n",
    "\n",
    "        #padding zero upto maximum length\n",
    "        train    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "        val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "        #size of vocabulary ( +1 for padding token)\n",
    "        x_voc   =  x_tokenizer.num_words + 1\n",
    "        #print(x_tokenizer.word_counts[text],len(train))\n",
    "        return x_voc, train, val\n",
    "            \n",
    "        \n",
    "        \n",
    "    def cleanedData(self):\n",
    "        text_word_count = []\n",
    "        summary_word_count = []\n",
    "        max_text_len = 0\n",
    "        max_summary_len = 0\n",
    "\n",
    "        # populate the lists with sentence lengths\n",
    "        for i in text_class.data['cleaned_text']:\n",
    "              text_word_count.append(len(i.split()))\n",
    "        \n",
    "\n",
    "        for i in text_class.data['cleaned_summary']:\n",
    "              summary_word_count.append(len(i.split()))\n",
    "\n",
    "        length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "        max_summary_len = self.majority_length_data(summary_word_count, self.cleaned_summary)\n",
    "        max_text_len = self.majority_length_data(text_word_count, self.cleaned_text)\n",
    "        print(max_summary_len)\n",
    "        print(max_text_len)\n",
    "        \n",
    "        cleaned_text =np.array(self.cleaned_text)\n",
    "        cleaned_summary=np.array(self.cleaned_summary)\n",
    "\n",
    "        short_text=[]\n",
    "        short_summary=[]\n",
    "\n",
    "        for i in range(len(cleaned_text)):\n",
    "            if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "                short_text.append(cleaned_text[i])\n",
    "                short_summary.append(cleaned_summary[i])\n",
    "\n",
    "        df=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
    "        df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "        x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True)\n",
    "        x_voc, x_tr, x_val = self.tokenizer(x_tr, x_val)\n",
    "        y_voc, y_tr, y_val = self.tokenizer(y_tr, y_val)\n",
    "        return text_word_count, summary_word_count, max_summary_len, max_text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "13383c43",
   "metadata": {
    "id": "13383c43",
    "outputId": "beb3ec1c-eb83-4942-a288-a52bedd53692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anusseth\\AppData\\Local\\Temp\\ipykernel_16876\\2259013371.py:17: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  newString = BeautifulSoup(newString, \"lxml\").text\n",
      "C:\\Users\\anusseth\\AppData\\Local\\Temp\\ipykernel_16876\\2259013371.py:17: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  newString = BeautifulSoup(newString, \"lxml\").text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 291, 1: 15077, 2: 28406, 3: 19570, 4: 13223, 5: 6264, 6: 3170, 7: 1404, 8: 531, 9: 232, 10: 129, 11: 63, 12: 37, 13: 15, 14: 3, 15: 8, 16: 2}\n",
      "84004\n",
      "{2: 3, 4: 16, 5: 96, 6: 242, 7: 499, 8: 983, 9: 1554, 10: 2116, 11: 2421, 12: 2709, 13: 2832, 14: 2744, 15: 2695, 16: 2594, 17: 2532, 18: 2390, 19: 2247, 20: 2210, 21: 2075, 22: 2043, 23: 1920, 24: 1906, 25: 1757, 26: 1793, 27: 1653, 28: 1634, 29: 1522, 30: 1472, 31: 1383, 32: 1385, 33: 1268, 34: 1228, 35: 1232, 36: 1134, 37: 1043, 38: 1059, 39: 1009, 40: 998, 41: 1012, 42: 968, 43: 868, 44: 804, 45: 843, 46: 777, 47: 767, 48: 709, 49: 691, 50: 679, 51: 613, 52: 572, 53: 566, 54: 574, 55: 575, 56: 482, 57: 536, 58: 506, 59: 457, 60: 477, 61: 436, 62: 438, 63: 422, 64: 395, 65: 404, 66: 392, 67: 358, 68: 362, 69: 317, 70: 295, 71: 304, 72: 310, 73: 303, 74: 278, 75: 238, 76: 251, 77: 237, 78: 232, 79: 225, 80: 193, 81: 206, 82: 210, 83: 197, 84: 210, 85: 204, 86: 212, 87: 173, 88: 159, 89: 179, 90: 132, 91: 144, 92: 165, 93: 139, 94: 136, 95: 158, 96: 130, 97: 123, 98: 138, 99: 133, 100: 142, 101: 123, 102: 104, 103: 121, 104: 128, 105: 118, 106: 100, 107: 109, 108: 98, 109: 99, 110: 77, 111: 83, 112: 90, 113: 87, 114: 80, 115: 78, 116: 83, 117: 68, 118: 64, 119: 69, 120: 53, 121: 73, 122: 69, 123: 63, 124: 68, 125: 56, 126: 49, 127: 59, 128: 53, 129: 74, 130: 46, 131: 47, 132: 63, 133: 37, 134: 51, 135: 52, 136: 40, 137: 40, 138: 47, 139: 36, 140: 44, 141: 39, 142: 47, 143: 44, 144: 35, 145: 27, 146: 33, 147: 30, 148: 43, 149: 44, 150: 34, 151: 34, 152: 19, 153: 32, 154: 36, 155: 26, 156: 30, 157: 23, 158: 23, 159: 30, 160: 24, 161: 23, 162: 27, 163: 17, 164: 24, 165: 20, 166: 25, 167: 24, 168: 23, 169: 17, 170: 26, 171: 19, 172: 23, 173: 21, 174: 14, 175: 23, 176: 24, 177: 18, 178: 17, 179: 14, 180: 10, 181: 22, 182: 16, 183: 15, 184: 10, 185: 18, 186: 13, 187: 10, 188: 19, 189: 14, 190: 16, 191: 8, 192: 9, 193: 8, 194: 11, 195: 12, 196: 15, 197: 14, 198: 12, 199: 9, 200: 10, 201: 13, 202: 17, 203: 8, 204: 10, 205: 15, 206: 9, 207: 8, 208: 11, 209: 10, 210: 7, 211: 6, 212: 7, 213: 6, 214: 9, 215: 10, 216: 5, 217: 6, 218: 8, 219: 7, 220: 7, 221: 7, 222: 6, 223: 7, 224: 6, 225: 9, 226: 5, 227: 9, 228: 3, 229: 8, 230: 5, 231: 9, 232: 3, 233: 4, 234: 6, 235: 5, 236: 5, 237: 4, 238: 1, 239: 2, 240: 2, 241: 15, 242: 5, 243: 9, 244: 3, 245: 8, 246: 9, 247: 6, 248: 7, 249: 1, 250: 7, 251: 6, 252: 1, 253: 4, 254: 5, 255: 4, 256: 5, 257: 6, 258: 8, 259: 3, 260: 1, 261: 3, 262: 1, 263: 3, 265: 3, 266: 3, 267: 2, 268: 2, 269: 2, 270: 3, 271: 6, 272: 4, 273: 6, 274: 3, 275: 2, 276: 2, 277: 1, 278: 4, 279: 4, 280: 2, 281: 2, 282: 2, 283: 6, 284: 1, 285: 4, 286: 3, 287: 6, 288: 1, 289: 6, 290: 1, 291: 2, 292: 1, 293: 2, 294: 1, 295: 1, 296: 3, 297: 4, 298: 2, 299: 2, 300: 2, 301: 3, 302: 1, 303: 1, 304: 1, 305: 2, 307: 1, 308: 2, 309: 2, 311: 1, 312: 3, 313: 1, 314: 1, 315: 1, 316: 3, 317: 3, 320: 1, 321: 3, 322: 1, 323: 2, 324: 1, 325: 2, 326: 3, 327: 2, 328: 2, 329: 3, 330: 2, 332: 2, 333: 2, 334: 1, 335: 2, 336: 2, 338: 1, 339: 2, 340: 3, 342: 1, 344: 1, 345: 1, 348: 3, 350: 1, 351: 2, 354: 1, 357: 2, 361: 1, 362: 1, 364: 1, 366: 1, 372: 1, 373: 1, 374: 1, 385: 1, 390: 2, 393: 1, 394: 1, 395: 2, 400: 2, 401: 1, 402: 1, 403: 2, 404: 2, 406: 1, 411: 1, 413: 1, 416: 3, 417: 2, 419: 1, 420: 1, 424: 1, 436: 1, 437: 1, 440: 1, 441: 1, 449: 1, 451: 1, 454: 1, 455: 1, 458: 1, 459: 1, 460: 1, 469: 1, 471: 1, 480: 1, 487: 1, 490: 1, 497: 1, 505: 1, 509: 1, 519: 1, 530: 1, 535: 1, 539: 1, 542: 1, 557: 2, 560: 1, 606: 1, 614: 1, 617: 1, 631: 1, 698: 1, 844: 1, 855: 1, 859: 1, 1236: 1}\n",
      "84004\n",
      "6\n",
      "104\n",
      "% of rare words in vocabulary: 71.36603637348256\n",
      "Total Coverage of rare words: 2.2052559981298776\n",
      "% of rare words in vocabulary: 76.49653434152489\n",
      "Total Coverage of rare words: 4.721348808175155\n"
     ]
    }
   ],
   "source": [
    "text_class = Text_Summarization(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9cdb5119",
   "metadata": {
    "id": "9cdb5119",
    "outputId": "5443cec7-7b2d-46f5-a806-d9dfd7d628b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                Summary  \\\n",
       "0   1  Good Quality Dog Food   \n",
       "1   2      Not as Advertised   \n",
       "2   3  \"Delight\" says it all   \n",
       "3   4         Cough Medicine   \n",
       "4   5            Great taffy   \n",
       "\n",
       "                                                Text  \\\n",
       "0  I have bought several of the Vitality canned d...   \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  This is a confection that has been around a fe...   \n",
       "3  If you are looking for the secret ingredient i...   \n",
       "4  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                        cleaned_text        cleaned_summary  \n",
       "0  bought several vitality canned dog food produc...  good quality dog food  \n",
       "1  product arrived labeled jumbo salted peanuts p...             advertised  \n",
       "2  confection around centuries light pillowy citr...           delight says  \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine  \n",
       "4  great taffy great price wide assortment yummy ...            great taffy  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_class.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e3bdb638",
   "metadata": {
    "id": "e3bdb638"
   },
   "outputs": [],
   "source": [
    "# text_word_count = []\n",
    "# summary_word_count = []\n",
    "# max_text_len = 0\n",
    "# max_summary_len = 0\n",
    "\n",
    "# # populate the lists with sentence lengths\n",
    "# for i in text_class.data['cleaned_text']:\n",
    "#       text_word_count.append(len(i.split()))\n",
    "        \n",
    "\n",
    "# for i in text_class.data['cleaned_summary']:\n",
    "#       summary_word_count.append(len(i.split()))\n",
    "\n",
    "# length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "# # max_text_len = max(text_word_count)\n",
    "# # max_summary_len = max(summary_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b7b8061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_summary_len = text_class.majority_length_data(text_class.summary_word_count,text_class.cleaned_summary)\n",
    "# max_text_len = text_class.majority_length_data(text_class.text_word_count,text_class.cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a5a414a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(max_summary_len)\n",
    "# print(max_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8eb11150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86001\n",
      "0.9725869380831212\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in text_class.data['cleaned_summary']:\n",
    "    if(len(i.split())<=max_summary_len):\n",
    "        cnt=cnt+1\n",
    "print (cnt)\n",
    "print(cnt/len(text_class.data['cleaned_summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "42f0e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_text =np.array(cleaned_text)\n",
    "# cleaned_summary=np.array(cleaned_summary)\n",
    "\n",
    "# short_text=[]\n",
    "# short_summary=[]\n",
    "\n",
    "# for i in range(len(cleaned_text)):\n",
    "#     if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "#         short_text.append(cleaned_text[i])\n",
    "#         short_summary.append(cleaned_summary[i])\n",
    "\n",
    "# df=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
    "# df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "# x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c8a23b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  \\\n",
      "0      bought several vitality canned dog food produc...   \n",
      "1      product arrived labeled jumbo salted peanuts p...   \n",
      "2      confection around centuries light pillowy citr...   \n",
      "3      looking secret ingredient robitussin believe f...   \n",
      "4      great taffy great price wide assortment yummy ...   \n",
      "...                                                  ...   \n",
      "82068               love noodle little spicy wife perfct   \n",
      "82069                 love buy another box done last one   \n",
      "82070  late father law used rating system meals parti...   \n",
      "82071  favorite brand korean ramen spicy used eating ...   \n",
      "82072  like noodles although say spicy somewhat under...   \n",
      "\n",
      "                     summary  \n",
      "0      good quality dog food  \n",
      "1                 advertised  \n",
      "2               delight says  \n",
      "3             cough medicine  \n",
      "4                great taffy  \n",
      "...                      ...  \n",
      "82068             good stuff  \n",
      "82069                  yummy  \n",
      "82070            tastes like  \n",
      "82071            great ramen  \n",
      "82072                  spicy  \n",
      "\n",
      "[82073 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "07e0760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "# # x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "59c9de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# #prepare a tokenizer for reviews on training data\n",
    "# x_tokenizer = Tokenizer()\n",
    "# x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d053058e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 71.36603637348256\n",
      "Total Coverage of rare words: 2.2052559981298776\n"
     ]
    }
   ],
   "source": [
    "# thresh=6\n",
    "\n",
    "# cnt=0\n",
    "# tot_cnt=0\n",
    "# freq=0\n",
    "# tot_freq=0\n",
    "\n",
    "# for key,value in x_tokenizer.word_counts.items():\n",
    "#     tot_cnt=tot_cnt+1\n",
    "#     tot_freq=tot_freq+value\n",
    "#     if(value<thresh):\n",
    "#         cnt=cnt+1\n",
    "#         freq=freq+value\n",
    "\n",
    "# print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "# print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c5163512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #prepare a tokenizer for reviews on training data\n",
    "# x_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
    "# x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# #convert text sequences into integer sequences\n",
    "# x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr)\n",
    "# x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "\n",
    "# #padding zero upto maximum length\n",
    "# x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "# x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "# #size of vocabulary ( +1 for padding token)\n",
    "# x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7ad31fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 76.49653434152489\n",
      "Total Coverage of rare words: 4.721348808175155\n"
     ]
    }
   ],
   "source": [
    "# #prepare a tokenizer for reviews on training data\n",
    "# y_tokenizer = Tokenizer()\n",
    "# y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "\n",
    "# thresh=6\n",
    "\n",
    "# cnt=0\n",
    "# tot_cnt=0\n",
    "# freq=0\n",
    "# tot_freq=0\n",
    "\n",
    "# for key,value in y_tokenizer.word_counts.items():\n",
    "#     tot_cnt=tot_cnt+1\n",
    "#     tot_freq=tot_freq+value\n",
    "#     if(value<thresh):\n",
    "#         cnt=cnt+1\n",
    "#         freq=freq+value\n",
    "\n",
    "# print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "# print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n",
    "\n",
    "# #prepare a tokenizer for reviews on training data\n",
    "# y_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
    "# y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "# #convert text sequences into integer sequences\n",
    "# y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr)\n",
    "# y_val_seq   =   y_tokenizer.texts_to_sequences(y_val)\n",
    "\n",
    "# #padding zero upto maximum length\n",
    "# y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "# y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "# #size of vocabulary\n",
    "# y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c18220fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73865, 73865)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sostok'],len(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c529f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
